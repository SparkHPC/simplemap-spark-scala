/*
 * This generates bash scripts for testing performance across various combinations 
 * of parameters (e.g. nodes, partitions, blocks and block size).
 *
 * The resulting scripts can be submitted to Cobalt via qsub. 
 *
 * The scripts start Apache Spark and run the assembly all-in-one.
 */

package edu.luc.cs

import scala.util.{ Try, Success, Failure }
import java.io._

object GenerateBashScripts {

  val cores = 12

  def main(args: Array[String]) {
    val scripts = generate
    scripts foreach saveFile
  }

  case class Script(filename: String, body: String)

  def saveFile(script: Script): Unit = {
    val scriptDir = new File(".", "scripts")
    Try { scriptDir.mkdir }

    if (scriptDir.exists) {
      val file = new File(scriptDir, script.filename)
      val bw = new BufferedWriter(new FileWriter(file))
      bw.write(script.body)
      bw.close
    } else {
      println("Cannot create script directory. No scripts written.")
    }
  }

  // generate shell scripts to run Spark experiments
  def generate(): Iterator[Script] = {
    for {
      nodes <- List(1, 4, 8, 16, 32, 64, 120).iterator
      nparts <- List(nodes * cores)
      blocks <- List(1000, 2000)
      blockSize <- List(10, 100, 1000)
    } yield {
      val fileName = s"do-simplemap-$nodes-$nparts-$blocks-$blockSize.sh"

      val scriptHeader = s"""#!/bin/bash
      |# Generated by Cobalt GenerateBasScripts
      |#
      |# Parameters (for debugging purposes):
      |#
      |# nodes = $nodes
      |# nparts = $nparts
      |# blocks = $blocks
      |# blockSize = $blockSize
      |# cores = $cores
      |#""".stripMargin

      val startSpark = s"""
      |#
      |# Start Apache Spark
      |#
      |
      |JOB_LOG=$$HOME/logs/$$COBALT_JOBID.txt
      |
      |pushd $$HOME/code/spark
      |cat $$COBALT_NODEFILE > conf/slaves
      |cat $$COBALT_NODEFILE >> $$JOB_LOG
      |./sbin/start-all.sh
      |NODES=`wc -l conf/slaves | cut -d" " -f1`
      |popd
      |
      |MASTER=`hostname`
      |
      |echo "# Spark is now running with $$NODES workers:" >> $$JOB_LOG
      |echo "#"
      |echo "export SPARK_STATUS_URL=http://$$MASTER.cooley.pub.alcf.anl.gov:8000" >> $$JOB_LOG
      |echo "export SPARK_MASTER_URI=spark://$$MASTER:7077" >> $$JOB_LOG
      |
      |SPARK_MASTER_URI=spark://$$MASTER:7077
      |SPARK_HOME=$$HOME/code/spark
      |
      |#
      |# Done Initializing Apache Spark
      |#""".stripMargin

      val submitSparkJob = s"""
      |#
      |# Submit Application on Spark
      |#
      |
      |ASSEMBLY=target/scala-2.10/simplemap-spark-scala-assembly-1.0.jar
      |if [ -f "$$ASSEMBLY" ]; then
      |
      |   echo "Running: "$$SPARK_HOME/bin/spark-submit \\
      |      --master $$SPARK_MASTER_URI $$ASSEMBLY \\
      |      --blocks $blocks --block_size $blockSize --nodes $nodes \\
      |      --nparts $nparts --cores $cores \\
      |      --json --xml >> $$JOB_LOG
      |
      |   $$SPARK_HOME/bin/spark-submit \\
      |      --master $$SPARK_MASTER_URI $$ASSEMBLY \\
      |      --blocks $blocks --block_size $blockSize --nodes $nodes \\
      |      --nparts $nparts --cores $cores \\
      |      --json --xml >> $$JOB_LOG
      |else
      |   echo "Could not find Scala target assembly. No experiments run." >> $$JOB_LOG
      |fi
      |
      |#
      |# Done Submitting Application on Spark
      |#
      |""".stripMargin

      Script(fileName, scriptHeader + startSpark + submitSparkJob)
    }
  }
}
