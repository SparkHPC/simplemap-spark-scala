/*
 * This generates bash scripts for testing performance across various combinations 
 * of parameters (e.g. nodes, partitions, blocks and block size).
 *
 * The resulting scripts can be submitted to Cobalt via qsub. 
 *
 * The scripts start Apache Spark and run the assembly all-in-one.
 */

package dataflows.spark

import scala.util.{ Try, Success, Failure }
import java.io._

object GenerateBashScripts {

  val cores = 12

  def main(args: Array[String]) {
    val scripts = generate
    scripts foreach saveFile
  }

  case class Script(dir: String, filename: String, body: String)

  def saveFile(script: Script): Unit = {
    val scriptDir = new File(script.dir)
    Try { scriptDir.mkdirs }

    if (scriptDir.exists) {
      val file = new File(scriptDir, script.filename)
      val bw = new BufferedWriter(new FileWriter(file))
      bw.write(script.body)
      bw.close
    } else {
      println("Cannot create script directory. No scripts written.")
    }
  }

  // generate shell scripts to run Spark experiments
  def generate(): Iterator[Script] = {
    val scriptBaseDir = new File(".", "qscripts.d")
    for {
      // blocks are allocated in 1MB chunks in the benchmark (1GB to 32GB here)
      blockSize <- List(1, 4, 8, 16, 32).map(gb => gb * 1024).iterator

      // nodes on Cooley (we stop at 100 since it is almost impossible to get 120+)
      nodes <- List(1, 4, 8, 16, 32, 64, 100)

      // partition multiplier (leave at 1 for now unless you want to spill more data)
      nparts <- List(1)

      // blocks (this gives us one block per partition when nparts=1)
      blocks <- List(nparts * nodes * cores)
    } yield {
      val scriptDir = new File(scriptBaseDir, s"$nodes")
      val filename = s"do-simplemap-${nodes}nodes-${nparts}parts-${blocks}blocks-${blockSize}MB.sh"

      val scriptHeader = s"""#!/bin/bash
      |# Generated by Cobalt GenerateBasScripts
      |#
      |# Parameters (for debugging purposes):
      |#
      |# nodes = $nodes
      |# nparts = $nparts
      |# blocks = $blocks
      |# blockSize = $blockSize
      |# cores = $cores
      |#""".stripMargin

      val startSpark = s"""
      |#
      |# Start Apache Spark
      |#
      |
      |JOB_LOG=$$HOME/logs/$$COBALT_JOBID.txt
      |JOB_JSON=$$HOME/logs/$$COBALT_JOBID.json
      |JOB_XML=$$HOME/logs/$$COBALT_JOBID.xml
      |
      |pushd $$HOME/code/spark
      |cat $$COBALT_NODEFILE > conf/slaves
      |cat $$COBALT_NODEFILE >> $$JOB_LOG
      |./sbin/start-all.sh
      |NODES=`wc -l conf/slaves | cut -d" " -f1`
      |popd
      |
      |MASTER=`hostname`
      |
      |echo "# Spark is now running with $$NODES workers:" >> $$JOB_LOG
      |echo "#"
      |echo "export SPARK_STATUS_URL=http://$$MASTER.cooley.pub.alcf.anl.gov:8000" >> $$JOB_LOG
      |echo "export SPARK_MASTER_URI=spark://$$MASTER:7077" >> $$JOB_LOG
      |
      |SPARK_MASTER_URI=spark://$$MASTER:7077
      |SPARK_HOME=$$HOME/code/spark
      |
      |#
      |# Done Initializing Apache Spark
      |#""".stripMargin

      val submitSparkJob = s"""
      |#
      |# Submit Application on Spark
      |#
      |
      |for ASSEMBLY in $$(find . -type f -name simplemap-spark-scala-assembly*.jar)
      |do
      |   echo "Running: "$$SPARK_HOME/bin/spark-submit \\
      |      --master $$SPARK_MASTER_URI $$ASSEMBLY \\
      |      --generate --blocks $blocks --block_size $blockSize --nodes $nodes \\
      |      --nparts $nparts --cores $cores \\
      |      --json $$JOB_JSON >> $$JOB_LOG
      |
      |   $$SPARK_HOME/bin/spark-submit \\
      |      --master $$SPARK_MASTER_URI $$ASSEMBLY \\
      |      --generate --blocks $blocks --block_size $blockSize --nodes $nodes \\
      |      --nparts $nparts --cores $cores \\
      |      --json $$JOB_JSON >> $$JOB_LOG
      |done
      |
      |#
      |# Done Submitting Application on Spark
      |#
      |""".stripMargin

      Script(scriptDir.toString, filename, scriptHeader + startSpark + submitSparkJob)
    }
  }
}
